{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to summarize my data collection and aggregation process. My data has three sources:\n",
    "* [Kaggle Play-by-Play Data](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016)\n",
    "* [NFL Teams](https://gist.githubusercontent.com/cnizzardini/13d0a072adb35a0d5817/raw/4b555e084e8cec673dc587555008607fb06c6a60/nfl_teams.csv)\n",
    "* [NFLWeather.com](http://www.nflweather.com/en/archive)\n",
    "\n",
    "**Please Note: Restarting this notebook will cause errors in the code since all the data is not stored locally.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "[Play-by-Play Data](#Play-by-Play-Data)<br>\n",
    "[Weather Data](#Weather-Data)<br>\n",
    "[Joining Dataframes](#Joining-Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 999999)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play-by-Play Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned above, the play-by-play dataset can be accessed on [Kaggle](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016). The file can be downloaded as a zip file. When unzipped, the file is a whopping 700 MB (not enormous but big enough where I don't want to store it locally). I uploaded the file onto a S3 server on AWS. My AWS key and secret key are not listed below for security purposes. \n",
    "<br>\n",
    "<br>\n",
    "To run this notebook without uploading the file onto AWS, the easiest thing is to take these steps:\n",
    "* Download dataset from link above\n",
    "* Unzip file\n",
    "* Run this code `df = pd.read_csv('file_name.csv')` adjust 'file_name.csv' to the file path of your unzipped csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=False,key='AWS KEY',secret='AWS SECRET KEY')\n",
    "\n",
    "key = 'NFL Play by Play 2009-2018 (v5).csv'\n",
    "bucket = 'nfl-play-by-play-capstone'\n",
    "\n",
    "df = pd.read_csv(fs.open('{}/{}'.format(bucket, key),\n",
    "                         mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below works in a jupyter notebook, but this directory also contains get_nfl_weather.py that can be run in the command line. The first function is just a progress bar, and the second function is the actual scrape. It takes two inputs: range of seasons (min 2009, max 2018) and a range of weeks (min 1, max 17). The main difference betweent the code below and the script is the location of the csv: the code below saves it in my datasets folder while the script saves the csv in your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nfl_weather(year_range:range,week_range:range):\n",
    "    \n",
    "    years = list(year_range)\n",
    "    weeks = list(week_range)\n",
    "    \n",
    "    l = len(years)\n",
    "    \n",
    "    games = []\n",
    "    \n",
    "    printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        for week in weeks:\n",
    "        \n",
    "            url = f'http://www.nflweather.com/en/week/{year}/week-{week}/'\n",
    "            \n",
    "            check = requests.get(url).status_code\n",
    "            \n",
    "            if check == 200:\n",
    "                \n",
    "                url = url\n",
    "            \n",
    "            else: \n",
    "                \n",
    "                url = f'http://www.nflweather.com/en/week/{year}/week-{week}-2/'\n",
    "                \n",
    "            req = requests.get(url).text\n",
    "\n",
    "            soup = BeautifulSoup(req,'lxml')\n",
    "\n",
    "            for row in range(len(soup.find('table').find_all('tr')[1::1])):\n",
    "                \n",
    "                game = {}\n",
    "                data = soup.find('table').find_all('tr')[1::1][row]\n",
    "                \n",
    "                try:\n",
    "                    away = data.find_all('a')[0].text\n",
    "                    home = data.find_all('a')[3].text\n",
    "                    forecast = data.find_all('td', class_ ='text-center')[5].text\n",
    "                    wind = data.find_all('td', class_ ='text-center')[6].text\n",
    "                    game['away'] = away\n",
    "                    game['home'] = home\n",
    "                    game['forecast'] = forecast.strip('\\n').strip('\\n ')\n",
    "                    game['wind'] = wind\n",
    "                    game['year'] = year\n",
    "                    game['week'] = week\n",
    "                    \n",
    "                except IndexError:pass\n",
    "                \n",
    "                games.append(game)\n",
    "                \n",
    "            time.sleep(1)\n",
    "            \n",
    "        printProgressBar(i + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "                \n",
    "    df = pd.DataFrame(games)\n",
    "    df.to_csv(f\"../datasets/nfl_weather_{min(years)}_to_{max(years)}_weeks_{min(weeks)}_to_{max(weeks)}.csv\",\n",
    "              index=False)\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would run this code below to execute the function in the notebook.\n",
    "\n",
    "```python \n",
    "weather = get_nfl_weather(range(2009,2019),range(1,18))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I am going to join the weather forecast for each game to the play-by-play dataframe. In order to do this, I first need to find an intermediary dataframe that has both team names and abbreviations since weather uses names and play-by-play uses abbreviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../datasets/nfl_weather_2009_to_2018_weeks_1_to_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('https://gist.githubusercontent.com/cnizzardini/'+\n",
    "                    '13d0a072adb35a0d5817/raw/4b555e084e8cec673dc587555008607fb06c6a60/nfl_teams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check if we have the same number of games in weather and play-by-play. I also want to check that we have 2560 games in total (256 regular season games a year, 10 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2526, 2559, 2560)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['game_id'].nunique(),weather.shape[0],256*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we have more games in weather than in the play-by-play dataframe. Weather seems to be missing one game, and play-by-play is missing 34 games. I will come back to this when I join the two dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to join weather and play-by-play data, I will need to manipulate the teams dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_abbreviations = list(teams['Abbreviation'].unique())\n",
    "df_abbreviations = list(df[['posteam']].sort_values('posteam')['posteam'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to check if the teams dataframe is missing any abbreviations used in the play-by-play dataframe, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAC\n",
      "LA\n",
      "LAC\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for i in df_abbreviations:\n",
    "    if i not in teams_abbreviations:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in teams_abbreviations:\n",
    "    if i not in df_abbreviations:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inconsistency arises for three teams. LA Rams were formerly St Louis Rams. LA Chargers were formerly San Diego Chargers. I was not sure why JAC & JAX are used in the df, so I looked into it. There was actually a dispute for changing the abbreviation that you can read about [here](https://www.espn.com/blog/jacksonville-jaguars/post/_/id/111/jags-fans-win-abbreviation-battle). I need to add these to the team dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams.append([{'ID':teams[teams['Abbreviation'] == 'STL']['ID'].values[0],'Name':'Los Angeles Rams',\n",
    "               'Abbreviation':'LA','Conference':teams[teams['Abbreviation'] == 'STL']['Conference'].values[0],\n",
    "               'Division':teams[teams['Abbreviation'] == 'STL']['Division'].values[0]},\n",
    "              {'ID':teams[teams['Abbreviation'] == 'SD']['ID'].values[0],'Name':'Los Angeles Chargers',\n",
    "               'Abbreviation':'LAC','Conference':teams[teams['Abbreviation'] == 'SD']['Conference'].values[0],\n",
    "               'Division':teams[teams['Abbreviation'] == 'SD']['Division'].values[0]},\n",
    "              {'ID':teams[teams['Abbreviation'] == 'JAX']['ID'].values[0],'Name':'Jacksonville Jaguars',\n",
    "               'Abbreviation':'JAC','Conference':teams[teams['Abbreviation'] == 'JAX']['Conference'].values[0],\n",
    "               'Division':teams[teams['Abbreviation'] == 'JAX']['Division'].values[0]}],\n",
    "             ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to isolate the name for each team from the location since the weather dataframe uses the name. I then make a dictionary with the abbreviations as the keys and save the updated teams table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams['team'] = teams['Name'].str.split().str[-1]\n",
    "teams_dict = dict(zip(teams['Abbreviation'],teams['team']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.to_csv('../datasets/nfl_teams_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to map the dictionary to the play-by-play dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['away_team_name'] = df['away_team'].map(teams_dict)\n",
    "df['home_team_name'] = df['home_team'].map(teams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before the join is isolating the seasons for each game. I will be joining on home, away, and season. I could not just pull out the year for each game because every year some games in a season are played in January of the next year. For example, a game played in January of 2017 is actually a 2016 season game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = {range(200909,201002):2009, range(201009,201102):2010, range(201109,201202):2011, range(201209,201302):2012,\n",
    "          range(201309,201402):2013, range(201409,201502):2014, range(201509,201602):2015, range(201609,201702): 2016,\n",
    "          range(201709,201802):2017, range(201809,201902):2018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'] = df['game_id'].astype(str).str[0:6].astype(int)\n",
    "df['season'] = df['season'].apply(lambda x: next((v for k, v in seasons.items() if x in k), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I merge the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_by_play = pd.merge(weather,df,left_on = ['away','home','year'], right_on = ['away_team_name', \n",
    "        'home_team_name','season'],how='left').drop(columns=['away','home','away_team_name','home_team_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate the discrepancies between the two dataframes that I noted earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_by_play['game_id'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151036</th>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>67f A Few Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346567</th>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>46f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449212</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>50f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449213</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>64f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449214</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449215</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>43f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449216</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>71f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449217</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>41f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449218</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>37f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449219</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>48f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449220</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449221</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>37f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449222</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>49f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449223</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>59f Partly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449224</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449225</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449226</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>45f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449227</th>\n",
       "      <td>16</td>\n",
       "      <td>2018</td>\n",
       "      <td>56f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449228</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>28f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449229</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>25f Mostly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449230</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>45f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449231</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>31f Partly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449232</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449233</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>35f Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449234</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>77f Mostly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449235</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>45f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449236</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>45f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449237</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>40f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449238</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>DOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449239</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>41f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449240</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>51f Partly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449241</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>62f Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449242</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>45f Mostly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449243</th>\n",
       "      <td>17</td>\n",
       "      <td>2018</td>\n",
       "      <td>53f Mostly Cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        week  year           forecast\n",
       "151036     7  2012   67f A Few Clouds\n",
       "346567    12  2016          46f Clear\n",
       "449212    16  2018          50f Clear\n",
       "449213    16  2018          64f Clear\n",
       "449214    16  2018               DOME\n",
       "449215    16  2018          43f Clear\n",
       "449216    16  2018          71f Clear\n",
       "449217    16  2018          41f Clear\n",
       "449218    16  2018       37f Overcast\n",
       "449219    16  2018          48f Clear\n",
       "449220    16  2018               DOME\n",
       "449221    16  2018          37f Clear\n",
       "449222    16  2018          49f Clear\n",
       "449223    16  2018  59f Partly Cloudy\n",
       "449224    16  2018               DOME\n",
       "449225    16  2018               DOME\n",
       "449226    16  2018       45f Overcast\n",
       "449227    16  2018       56f Overcast\n",
       "449228    17  2018       28f Overcast\n",
       "449229    17  2018  25f Mostly Cloudy\n",
       "449230    17  2018       45f Overcast\n",
       "449231    17  2018  31f Partly Cloudy\n",
       "449232    17  2018               DOME\n",
       "449233    17  2018       35f Overcast\n",
       "449234    17  2018  77f Mostly Cloudy\n",
       "449235    17  2018          45f Clear\n",
       "449236    17  2018          45f Clear\n",
       "449237    17  2018          40f Clear\n",
       "449238    17  2018               DOME\n",
       "449239    17  2018          41f Clear\n",
       "449240    17  2018  51f Partly Cloudy\n",
       "449241    17  2018          62f Clear\n",
       "449242    17  2018  45f Mostly Cloudy\n",
       "449243    17  2018  53f Mostly Cloudy"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_by_play[play_by_play['game_id'].isnull()][['week','year','forecast']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that the play-by-play data is missing 34 games. Below are the games that the play-by-play data is missing. It is mostly 2018 weeks 16-17 which admittedly, when I look back at the Kaggle source, it does say up to week 15, so that takes care of 32 missing games. There are also an additional two weeks that just seem to be missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2525"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_by_play['game_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join also got rid of the play-by-play data for the game that weather was missing since the number of unique games decreased by 1. At this point, I am okay with missing 3 games (two randomly from play-by-play & one randomly from nflweather.com) and the final two weeks of 2018. I will drop the rows for these games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_by_play = play_by_play.drop(list(play_by_play[play_by_play['game_id'].isnull()].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I uploaded the new file back to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=False,key='AWS KEY',secret='AWS SECRET KEY')\n",
    "\n",
    "bucket = 'nfl-play-by-play-capstone'\n",
    "\n",
    "with fs.open(f'{bucket}/nfl_play_by_play_with_weather_2009_2018.csv','w') as f:\n",
    "    play_by_play.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
